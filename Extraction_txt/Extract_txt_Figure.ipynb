{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image \n",
    "import requests \n",
    "from transformers import AutoModelForCausalLM \n",
    "from transformers import AutoProcessor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_phi3():\n",
    "    model_id = \"microsoft/Phi-3-vision-128k-instruct\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", trust_remote_code=True, torch_dtype=\"auto\",_attn_implementation='eager')\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "    return model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7a07643c014ca7a3d0e5b3e9a34570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ekalboussi\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "C:\\Users\\ekalboussi\\.cache\\huggingface\\modules\\transformers_modules\\microsoft\\Phi-3-vision-128k-instruct\\c45209e90a4c4f7d16b2e9d48503c7f3e83623ed\\image_embedding_phi3_v.py:197: UserWarning: Phi-3-V modifies `input_ids` in-place and the tokens indicating images will be removed after model forward. If your workflow requires multiple forward passes on the same `input_ids`, please make a copy of `input_ids` before passing it to the model.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ekalboussi\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:480: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image displays a man in a business suit with a tie, standing in front of a blurred background. There is text overlaid on the image, and a figure number '0.54' is visible in the top left corner.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "from PIL import Image\n",
    "\n",
    "model_id = \"microsoft/Phi-3-vision-128k-instruct\" \n",
    "\n",
    "# Charger le modèle avec les paramètres nécessaires\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", trust_remote_code=True, torch_dtype=\"auto\", _attn_implementation='eager') \n",
    "\n",
    "# Charger le processeur\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True) \n",
    "\n",
    "# Message pour l'image\n",
    "messages = [ \n",
    "    {\"role\": \"user\", \"content\": \"<|image_1|>\\nWhat is shown in this image?\"},  \n",
    "] \n",
    "\n",
    "# Utilisation du chemin d'accès corrigé avec une chaîne brute\n",
    "url =,
    "# Ouvrir l'image\n",
    "image = Image.open(url) \n",
    "\n",
    "# Créer l'invite\n",
    "prompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Préparer les entrées pour le modèle\n",
    "inputs = processor(prompt, [image], return_tensors=\"pt\").to(\"cuda:0\") \n",
    "\n",
    "# Paramètres de génération\n",
    "generation_args = { \n",
    "    \"max_new_tokens\": 500, \n",
    "    \"temperature\": 0.0, \n",
    "    \"do_sample\": False, \n",
    "} \n",
    "\n",
    "# Générer la réponse\n",
    "generate_ids = model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args) \n",
    "\n",
    "# Supprimer les tokens d'entrée\n",
    "generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
    "\n",
    "# Décoder et afficher la réponse\n",
    "response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0] \n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "835ab45710c4468d8a333af018d875a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ekalboussi\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\image_processing_auto.py:513: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\ekalboussi\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "C:\\Users\\ekalboussi\\.cache\\huggingface\\modules\\transformers_modules\\microsoft\\Phi-3-vision-128k-instruct\\c45209e90a4c4f7d16b2e9d48503c7f3e83623ed\\image_embedding_phi3_v.py:197: UserWarning: Phi-3-V modifies `input_ids` in-place and the tokens indicating images will be removed after model forward. If your workflow requires multiple forward passes on the same `input_ids`, please make a copy of `input_ids` before passing it to the model.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ekalboussi\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:480: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The French text from the image is: « 2021 a été une année de confirmation pour Carrefour. Celle, d'abord, de la bonne exécution de notre plan stratégique. [...] Celle, ensuite, de la force du modèle de croissance, ancré dans les nouvelles tendances de consommation, dont disposé aujourd'hui Carrefour [...]. Celle, enfin, du sens du service et de l'engagement sans faille. »\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "from PIL import Image\n",
    "\n",
    "model_id = \"microsoft/Phi-3-vision-128k-instruct\" \n",
    "\n",
    "# Charger le modèle avec une précision réduite\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", trust_remote_code=True, torch_dtype=\"float16\", _attn_implementation='eager') \n",
    "\n",
    "# Charger le processeur\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True) \n",
    "\n",
    "# Message pour l'extraction du texte\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"<|image_1|>\\nExtract the French text from this image.\"},  \n",
    "]\n",
    "\n",
    "# Chemin de l'image\n",
    "url = 
    "image = Image.open(url) \n",
    "\n",
    "# Créer l'invite\n",
    "prompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Préparer les entrées pour le modèle\n",
    "inputs = processor(prompt, [image], return_tensors=\"pt\").to(\"cuda:0\") \n",
    "\n",
    "# Paramètres de génération\n",
    "generation_args = { \n",
    "    \"max_new_tokens\": 500, \n",
    "    \"temperature\": 0.0, \n",
    "    \"do_sample\": False, \n",
    "} \n",
    "\n",
    "# Générer la réponse\n",
    "generate_ids = model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args) \n",
    "\n",
    "# Supprimer les tokens d'entrée\n",
    "generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
    "\n",
    "# Décoder et afficher la réponse\n",
    "response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0] \n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e671ba0243cb4e69a2451a2c7b4541b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image contains French text discussing different algorithms for measuring similarity, specifically in the context of ensembles of motifs and vectors. It mentions the importance of choosing the right algorithm based on the category of vectors being used and provides guidance on how to select the appropriate algorithm. The text also explains the concept of cosine similarity and its use in measuring the similarity between two vectors. Additionally, it touches on the use of cosine similarity in the context of document classification and the calculation of cosine similarity between two vectors. The text is part of a document or presentation related to information retrieval and is marked as confidential.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "from PIL import Image\n",
    "\n",
    "model_id = \"microsoft/Phi-3-vision-128k-instruct\" \n",
    "\n",
    "# Charger le modèle avec une précision réduite\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", trust_remote_code=True, torch_dtype=\"float16\", _attn_implementation='eager') \n",
    "\n",
    "# Charger le processeur\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True) \n",
    "\n",
    "# Message pour l'extraction du texte\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"<|image_1|>\\nExtract the French text from this image.\"},  \n",
    "]\n",
    "\n",
    "# Chemin de l'image\n",
    "url = ,
    "image = Image.open(url) \n",
    "\n",
    "# Créer l'invite\n",
    "prompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Préparer les entrées pour le modèle\n",
    "inputs = processor(prompt, [image], return_tensors=\"pt\").to(\"cuda:0\") \n",
    "\n",
    "# Paramètres de génération\n",
    "generation_args = { \n",
    "    \"max_new_tokens\": 500, \n",
    "    \"temperature\": 0.0, \n",
    "    \"do_sample\": False, \n",
    "} \n",
    "\n",
    "# Générer la réponse\n",
    "generate_ids = model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args) \n",
    "\n",
    "# Supprimer les tokens d'entrée\n",
    "generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
    "\n",
    "# Décoder et afficher la réponse\n",
    "response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0] \n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8081417753543578aa89fc653f0df98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'image montre un diagramme représentant différents algorithmes de mesure de similitude. Il y a des éléments textuels qui sont en français, tels que 'Mesure de similitude', 'Similarité de Jaccard', 'Similarité de Dice', 'Similarité de recouvrement', 'Similarité cosinus', 'Similarité cosinus douce', 'Travail avec des ensembles', 'Travail avec des vecteurs', et 'Travail avec des vecteurs douce'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "from PIL import Image\n",
    "\n",
    "model_id = \"microsoft/Phi-3-vision-128k-instruct\" \n",
    "\n",
    "# Charger le modèle avec une précision réduite\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", trust_remote_code=True, torch_dtype=\"float16\", _attn_implementation='eager') \n",
    "\n",
    "# Charger le processeur\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True) \n",
    "\n",
    "# Message pour l'extraction du texte\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"<|image_1|>\\nexplain figure3 in this image.\"},  \n",
    "]\n",
    "\n",
    "# Chemin de l'image\n",
    "url = 
    "\n",
    "# Créer l'invite\n",
    "prompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Préparer les entrées pour le modèle\n",
    "inputs = processor(prompt, [image], return_tensors=\"pt\").to(\"cuda:0\") \n",
    "\n",
    "# Paramètres de génération\n",
    "generation_args = { \n",
    "    \"max_new_tokens\": 500, \n",
    "    \"temperature\": 0.0, \n",
    "    \"do_sample\": False, \n",
    "} \n",
    "\n",
    "# Générer la réponse\n",
    "generate_ids = model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args) \n",
    "\n",
    "# Supprimer les tokens d'entrée\n",
    "generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
    "\n",
    "# Décoder et afficher la réponse\n",
    "response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0] \n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de4968ec4c047b4a750c4cbb65c1a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'extraction du texte français sous la figure 3 nécessite une compréhension de la langue française et de la structure de la figure. Voici une extraction possible :\n",
      "\n",
      "\"Il est possible de classer ces algorithmes en deux catégories : ceux qui opèrent sur des ensembles de mots et ceux qui travaillent avec des vecteurs. Dans notre étude, nous nous concentrons sur la deuxième catégorie, c'est-à-dire les algorithmes qui utilisent des vecteurs. Parmi les options disponibles, nous avons le choix entre la similarité cosinus et la similarité cosinus douce. Il est important de noter que le terme « similarité cosinus douce » est une traduction directe de « soft cosine similarity » en anglais. Pour obtenir des informations détaillées sur cette mesure, il est recommandé de se référer à son nom anglais, car il s'agit d'un outil relativement récent avec peu de documentation disponible. La similarité cosinus (ou cosine similarity en anglais) est une mesure couramment utilisée en analyse de données, en apprentissage automatique et en traitement du langage naturel pour évaluer la similarité entre deux vecteurs dans un espace multidimensionnel. La similarité cosinus mesure l'angle entre deux vecteurs dans l'espace latent engendré par nos embeddings de mots. Cette réponse sur le produit scalaire entre nos deux vecteurs normalisés qui permet de calculer l'angle entre nos vecteurs. Une similarité de 0 indique que nos vecteurs sont orthogonaux, de 1 qu'ils sont colinéaires de même sens et -1 qu'ils sont opposés (colinéaire de sens opposé). Donc plus l'angle entre les vecteurs est petit, plus la similarité est proche de 1 plus nos représentations sont proches et plus l'angle entre les vecteurs est grand plus la similarité est faible. Formellement, si nous avons deux vecteurs A et B, la similarité cosinus est calculée comme suit :\"\n",
      "\n",
      "\n",
      "Notez que cette extraction est une traduction de l'\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "from PIL import Image\n",
    "\n",
    "model_id = \"microsoft/Phi-3-vision-128k-instruct\" \n",
    "\n",
    "# Charger le modèle avec une précision réduite\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", trust_remote_code=True, torch_dtype=\"float16\", _attn_implementation='eager') \n",
    "\n",
    "# Charger le processeur\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True) \n",
    "\n",
    "# Message pour l'extraction du texte\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"<|image_1|>\\nExtract  the French text under the figure 3 in this images ?\"},  \n",
    "]\n",
    "\n",
    "# Chemin de l'image\n",
    "url =,
    "image = Image.open(url) \n",
    "\n",
    "# Créer l'invite\n",
    "prompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Préparer les entrées pour le modèle\n",
    "inputs = processor(prompt, [image], return_tensors=\"pt\").to(\"cuda:0\") \n",
    "\n",
    "# Paramètres de génération\n",
    "generation_args = { \n",
    "    \"max_new_tokens\": 500, \n",
    "    \"temperature\": 0.0, \n",
    "    \"do_sample\": False, \n",
    "} \n",
    "\n",
    "# Générer la réponse\n",
    "generate_ids = model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args) \n",
    "\n",
    "# Supprimer les tokens d'entrée\n",
    "generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
    "\n",
    "# Décoder et afficher la réponse\n",
    "response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0] \n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
